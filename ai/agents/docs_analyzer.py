# agents/docs_analyzer.py
import os
from typing import Dict, Any, Optional, List

from qdrant_client import QdrantClient
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

from core import config 
from ai.graphs.state_definition import LangGraphState 
from ai.tools.vector_db_retriever import retrieve_documents
from schemas.project_info import ProjectInfo

class DocsAnalyzer:
    def __init__(self, qdrant_client: QdrantClient):
        self.qdrant_client = qdrant_client
            
        self.llm = ChatOpenAI(
            model=config.DEFAULT_MODEL,
            temperature=0.2,
            max_tokens=2000,
            openai_api_key=config.OPENAI_API_KEY
        )
        
        self._load_prompt()
        self.parser = JsonOutputParser()

    def _load_prompt(self):
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        prompt_file_path = os.path.join(config.PROMPTS_BASE_DIR, "docs_analyze_prompt.md")
        try:
            with open(prompt_file_path, "r", encoding="utf-8") as f:
                prompt_template_str = f.read()
            self.prompt = PromptTemplate.from_template(prompt_template_str)
        except FileNotFoundError:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            self.prompt = PromptTemplate.from_template(
                "ì‚¬ìš©ìž ID {user_id} (ì´ë¦„: {user_name})ì˜ ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬ WBS({wbs_data})ì™€ ê´€ë ¨ëœ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ê³ , ê´€ë ¨ ìž‘ì—… ë§¤ì¹­ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:\n\n{documents}\n\në¶„ì„ ê¸°ì¤€ì¼: {target_date}"
            )

    def _get_retrieved_docs_list(self, state: LangGraphState) -> List[Dict]:
        """stateì—ì„œ retrieved_docs_listë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ì§ì ‘ ê²€ìƒ‰"""
        retrieved_docs_list = state.get("retrieved_docs_list")
        
        if retrieved_docs_list:
            print(f"DocsAnalyzer: stateì—ì„œ {len(retrieved_docs_list)}ê°œ ë¬¸ì„œ ìž¬ì‚¬ìš©")
            return retrieved_docs_list
        
        # stateì— ì—†ìœ¼ë©´ ì§ì ‘ ê²€ìƒ‰
        user_id = state.get("user_id")
        target_date = state.get("target_date")
        
        retrieved_docs_list = retrieve_documents(
            qdrant_client=self.qdrant_client,
            user_id=user_id,
            target_date_str=target_date,
        )
        
        return retrieved_docs_list

    def _count_unique_documents(self, retrieved_docs_list: List[Dict]) -> int:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³ ìœ í•œ ë¬¸ì„œì˜ ê°œìˆ˜ë¥¼ ê³„ì‚°"""
        unique_filenames = set()
        
        for doc_item in retrieved_docs_list:
            metadata = doc_item.get("metadata", {})
            filename = metadata.get("filename", metadata.get("title", "Unknown Document"))
            unique_filenames.add(filename)
        
        return len(unique_filenames)

    def _format_documents_for_analysis(self, retrieved_docs_list: List[Dict], user_id: str) -> str:
        """ë¬¸ì„œ ëª©ë¡ì„ ë¶„ì„ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not retrieved_docs_list:
            return "ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        documents_text_parts = []
        for doc_item in retrieved_docs_list:
            metadata = doc_item.get("metadata", {})
            filename = metadata.get("filename", metadata.get("title", "Unknown Document"))
            title = doc_item.get("title", "")
            author = metadata.get("author", user_id)  # ìž‘ì„±ìž ì—†ìœ¼ë©´ user_id ì‚¬ìš©
            
            documents_text_parts.append(
                f"íŒŒì¼ëª…: {filename}\nìž‘ì„±ìž: {author}\nì œëª©:\n{title}...\n---"
            )

        return "\n\n".join(documents_text_parts)
    
    def _analyze_docs_data_internal(
        self,
        user_id: str,
        user_name: Optional[str],
        target_date: str,
        wbs_data: Optional[dict],
        retrieved_docs_list: List[Dict],
        docs_quality_result: Optional[dict] = None,
        projects: List[ProjectInfo] = None,
    ) -> Dict[str, Any]:
        """ë‚´ë¶€ ë¬¸ì„œ ë¶„ì„ ë¡œì§"""
        print(f"DocsAnalyzer: ì‚¬ìš©ìž ID '{user_id}'ì˜ ë¬¸ì„œ {len(retrieved_docs_list)}ê°œ ë¶„ì„ ì‹œìž‘")
        
        # Unique ë¬¸ì„œ ê°œìˆ˜ ê³„ì‚°
        unique_count = self._count_unique_documents(retrieved_docs_list)

        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
        if not retrieved_docs_list:
            return {
                "user_id": user_id,
                "user_name": user_name or user_id,
                "date": target_date,
                "type": "docs",
                "docs_analysis": {
                    "matched_docs": [],
                    "unmatched_docs": []
                },
                "daily_reflection": {
                    "title": "ðŸ” ì¢…í•© ë¶„ì„ ë° í”¼ë“œë°±",
                    "analysis_limitations": "ë¶„ì„í•  ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "content": [
                        "ì´í‰: ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œê°€ ì—†ì–´ ì—…ë¬´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "ê°œì„  ì œì•ˆ: ë¬¸ì„œ ìž‘ì„± ë° ì—…ë¡œë“œ í”„ë¡œì„¸ìŠ¤ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                        "ì¶”ê°€ ì˜ê²¬: í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì„ ë¬¸ì„œë¡œ ê¸°ë¡í•˜ëŠ” ìŠµê´€ì„ ê¶Œìž¥í•©ë‹ˆë‹¤."
                    ]
                },
                "total_tasks": 0
            }

        # ë¬¸ì„œ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬
        documents_text = self._format_documents_for_analysis(retrieved_docs_list, user_id)
        wbs_data_str = str(wbs_data) if wbs_data else "WBS ì •ë³´ ì—†ìŒ"

        # LLM Chain êµ¬ì„±
        chain = (
            {
                "documents": lambda x: x["documents_text"],
                "wbs_data": lambda x: x["wbs_info"],
                "user_id": lambda x: x["in_user_id"],
                "user_name": lambda x: x["in_user_name"],
                "target_date": lambda x: x["in_target_date"],
                "docs_quality_result": lambda x: x["docs_quality_result"],
                "total_tasks": lambda x: x["in_total_tasks"],
                "projects": lambda x: x["projects"]
            }
            | self.prompt
            | self.llm
            | self.parser
        )

        try:
            result = chain.invoke({
                "in_user_id": user_id,
                "in_user_name": user_name or user_id,
                "in_target_date": target_date,
                "wbs_info": wbs_data_str,
                "documents_text": documents_text,
                "docs_quality_result": docs_quality_result or {},
                "in_total_tasks": unique_count,
                "projects": projects
            })
            
            # ê²°ê³¼ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
            if not isinstance(result, dict):
                raise ValueError("LLMì´ ì˜¬ë°”ë¥¸ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            return result
        
        except Exception as e:
            print(f"DocsAnalyzer: LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def analyze_documents(self, state: LangGraphState) -> LangGraphState:
        """ë©”ì¸ ë¬¸ì„œ ë¶„ì„ ë©”ì„œë“œ"""
        user_id = state.get("user_id")
        user_name = state.get("user_name")
        target_date = state.get("target_date")
        wbs_data = state.get("wbs_data")
        quality_result = state.get("documents_quality_result", {})
        projects = state.get("projects")
                
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ê²€ì¦
        if not user_id:
            error_msg = "DocsAnalyzer: user_idê°€ Stateì— ì œê³µë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."
            print(error_msg)
            return {"documents_analysis_result": {"error": error_msg, "type": "docs"}}
        
        if not target_date:
            error_msg = "DocsAnalyzer: target_dateê°€ Stateì— ì œê³µë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤."
            print(error_msg)
            return {"documents_analysis_result": {"error": error_msg, "type": "docs"}}

        # retrieved_docs_list ê°€ì ¸ì˜¤ê¸° (stateì—ì„œ ìž¬ì‚¬ìš© ë˜ëŠ” ì§ì ‘ ê²€ìƒ‰)
        retrieved_docs_list = self._get_retrieved_docs_list(state)

        # ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰
        analysis_result = self._analyze_docs_data_internal(
            user_id=user_id,
            user_name=user_name,
            target_date=target_date,
            wbs_data=wbs_data,
            retrieved_docs_list=retrieved_docs_list,
            docs_quality_result=quality_result,
            projects=projects
        )
        
        return {"documents_analysis_result": analysis_result}

    def __call__(self, state: LangGraphState) -> LangGraphState:
        return self.analyze_documents(state)