# agents/docs_quality_analyzer.py - JSON parser ì‚¬ìš© ë²„ì „
import os
import sys
import json
from typing import Dict, Any, List

from qdrant_client import QdrantClient
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from core import config 
from core.state_definition import LangGraphState 
from tools.vector_db_retriever import retrieve_documents, retrieve_documents_content


class DocsQualityAnalyzer:
    def __init__(self, qdrant_client: QdrantClient):
        self.qdrant_client = qdrant_client
        self.llm = ChatOpenAI(
            model=config.DEFAULT_MODEL,
            temperature=0.2,
            max_tokens=2000,
            openai_api_key=config.OPENAI_API_KEY
        )
        
        # JSON parser (docs_analyzer.py ìŠ¤íƒ€ì¼)
        self.json_parser = JsonOutputParser()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        self._load_prompts()
    
    def _load_prompts(self):
        """í”„ë¡¬í”„íŠ¸ .md íŒŒì¼ë“¤ì„ PromptTemplate ê°ì²´ë¡œ ë¡œë“œ"""
        try:
            # ì¤‘ìš”ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸
            importance_path = os.path.join(config.PROMPTS_BASE_DIR, "docs_importance_evaluation_prompt.md")
            with open(importance_path, "r", encoding="utf-8") as f:
                importance_content = f.read()
            
            # í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸
            quality_path = os.path.join(config.PROMPTS_BASE_DIR, "docs_quality_analyze_prompt.md")
            with open(quality_path, "r", encoding="utf-8") as f:
                quality_content = f.read()
            
            # PromptTemplate ê°ì²´ ìƒì„±
            self.importance_prompt = PromptTemplate.from_template(importance_content)
            self.quality_prompt = PromptTemplate.from_template(quality_content)
                
            print("âœ… í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
        except FileNotFoundError as e:
            print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {e}")

    def analyze_document_quality(self, state: LangGraphState) -> LangGraphState:
        """ë©”ì¸ ë¬¸ì„œ í’ˆì§ˆ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        print(f"DocsQualityAnalyzer: ì‚¬ìš©ìž ID '{state.get('user_id')}'ì˜ ë¬¸ì„œ í’ˆì§ˆ ë¶„ì„ ì‹œìž‘...")
        
        user_id = state.get("user_id")
        target_date = state.get("target_date")
        
        if not user_id:
            return {"documents_quality_result": {"error": "user_idê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}}

        # ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs_list = retrieve_documents(
            qdrant_client=self.qdrant_client,
            user_id=user_id,
            target_date_str=target_date,
        )
        
        if not retrieved_docs_list:
            return {"documents_quality_result": {"error": "ë¶„ì„í•  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}}

        print(f"âœ… {len(retrieved_docs_list)}ê°œ ë¬¸ì„œ ë°œê²¬")

        # ë¶„ì„ ì‹¤í–‰
        quality_results = self._analyze_quality_internal(retrieved_docs_list)

        return {"documents_quality_result": quality_results}

    def _analyze_quality_internal(self, retrieved_docs_list: List[Dict]) -> Dict[str, Any]:
        """ë‚´ë¶€ í’ˆì§ˆ ë¶„ì„ ë¡œì§"""
        try:
            # 1ë‹¨ê³„: ì¤‘ìš” ë¬¸ì„œ + í¬í•¨í•  ë‚´ìš© ì„ ë³„
            important_docs, required_contents = self._get_important_documents_and_contents(retrieved_docs_list)
            
            if not important_docs:
                return {"error": "ì¤‘ìš”í•œ ë¬¸ì„œê°€ ì„ ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
            
            print(f"âœ… ì¤‘ìš” ë¬¸ì„œ: {important_docs}")
            print(f"âœ… í•„ìš” ë‚´ìš©: {required_contents}")
            
            # 2ë‹¨ê³„: hybrid searchë¡œ ê´€ë ¨ content ê°€ì ¸ì˜¤ê¸°
            search_results = self._hybrid_search_for_quality(important_docs, required_contents)
            
            if not search_results:
                return {"error": "hybrid search ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            print(f"âœ… hybrid search: {len(search_results)}ê°œ chunk")
            
            # 3ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
            quality_results = self._evaluate_quality_by_file(search_results)
            print(f"âœ… í’ˆì§ˆ í‰ê°€: {len(quality_results)}ê°œ ë¬¸ì„œ")

            return quality_results
            
        except Exception as e:
            print(f"âš ï¸ í’ˆì§ˆ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def _get_important_documents_and_contents(self, retrieved_docs_list: List[Dict]) -> tuple:
        """ì¤‘ìš” ë¬¸ì„œì™€ í¬í•¨í•  ë‚´ìš© ì„ ë³„ (JSON parser ì‚¬ìš©)"""
        
        # ê³ ìœ  ë¬¸ì„œ ëª©ë¡ ìƒì„±
        unique_docs = {}
        for doc in retrieved_docs_list:
            filename = doc.get("metadata", {}).get("filename", "Unknown")
            if filename != "Unknown" and filename not in unique_docs:
                file_type = doc.get("metadata", {}).get("type", "Unknown")
                unique_docs[filename] = file_type
        
        doc_list = "\\n".join([f"{i}. {filename} ({file_type})" 
                             for i, (filename, file_type) in enumerate(unique_docs.items(), 1)])
        
        try:
            # Chain êµ¬ì„± (docs_analyzer.py ìŠ¤íƒ€ì¼)
            chain = (
                {
                    "doc_list": lambda x: x["input_doc_list"]
                }
                | self.importance_prompt
                | self.llm
                | self.json_parser
            )
            
            # Chain ì‹¤í–‰
            result = chain.invoke({
                "input_doc_list": doc_list
            })
            
            print(f"ðŸ” JSON íŒŒì‹± ê²°ê³¼: {result}")
            
            # JSONì—ì„œ ë°ì´í„° ì¶”ì¶œ
            important_docs = result.get("important_docs", [])
            contents_dict = result.get("contents", {})
            return important_docs, contents_dict

        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            # í´ë°±: ì²˜ìŒ 3ê°œ ë¬¸ì„œ
            fallback_docs = list(unique_docs.keys())[:3]
            fallback_contents = {doc: ["ë¬¸ì„œ ì™„ì„±ë„", "ê¸°ìˆ ì  ì •í™•ì„±", "ì‹¤ë¬´ í™œìš©ì„±"] for doc in fallback_docs}
            return fallback_docs, fallback_contents

    def _hybrid_search_for_quality(self, important_docs: List[str], required_contents: Dict[str, List[str]]) -> List[Dict]:
        """hybrid searchë¡œ í’ˆì§ˆ í‰ê°€ìš© content ê°€ì ¸ì˜¤ê¸°"""
        
        all_results = []
        
        for filename in important_docs:
            contents = required_contents.get(filename, ["ë¬¸ì„œ ì™„ì„±ë„"])
            
            try:
                # retrieve_documents_content ì‚¬ìš©
                results = retrieve_documents_content(
                    qdrant_client=self.qdrant_client,
                    document_list=[{"filename": filename}],
                    queries=contents,
                    top_k=3  # ê° ì¿¼ë¦¬ë‹¹ 3ê°œ
                )
                
                all_results.extend(results)
                print(f"  ðŸ“„ {filename}: {len(results)}ê°œ chunk ê²€ìƒ‰ë¨ (ì¿¼ë¦¬: {contents})")
                
            except Exception as e:
                print(f"  âš ï¸ {filename} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return all_results

    def _evaluate_quality_by_file(self, search_results: List[Dict]) -> Dict[str, Any]:
        """íŒŒì¼ë³„ë¡œ chunkë¥¼ ì¢…í•©í•˜ì—¬ í’ˆì§ˆ í‰ê°€"""
        
        # íŒŒì¼ë³„ë¡œ chunk ê·¸ë£¹í™”
        file_chunks = {}
        for chunk in search_results:
            filename = chunk.get("filename") or chunk.get("metadata", {}).get("filename", "Unknown")
            if filename not in file_chunks:
                file_chunks[filename] = []
            file_chunks[filename].append(chunk)
        
        file_evaluations = []
        
        for filename, chunks in file_chunks.items():
            try:
                # íŒŒì¼ì˜ ëª¨ë“  chunk content í•©ì¹˜ê¸°
                combined_content = "\\n\\n".join([
                    f"[Chunk {i+1}]: {chunk.get('page_content', '')[:400]}"
                    for i, chunk in enumerate(chunks)
                ])
                
                # í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì‘ë‹µ)
                quality_evaluation_chain = (
                    {
                        "filename": lambda x: x["filename"],
                        "combined_content": lambda x: x["combined_content"]
                    }
                    | self.quality_prompt # í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    | self.llm
                    | self.json_parser # JSON Output Parser ì ìš©
                )

                result_json = quality_evaluation_chain.invoke({
                    "filename": filename,
                    "combined_content": combined_content
                })

                file_evaluations.append({
                    "evaluation": result_json
                })
                
            except Exception as e:
                print(f"  âš ï¸ {filename} í‰ê°€ ì˜¤ë¥˜: {e}")

        return file_evaluations

    def __call__(self, state: LangGraphState) -> LangGraphState:
        return self.analyze_document_quality(state)
