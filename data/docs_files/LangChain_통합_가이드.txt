LangChain 프레임워크 통합 가이드
1. 개요 본 문서는 스마트팩토리 챗봇 시스템에서 RAG(Retrieval Augmented Generation) 기능을 구현하기 위한 LangChain 프레임워크 통합 방법을 설명합니다. LangChain은 LLM 기반 애플리케이션 개발을 위한 강력한 도구이며, 특히 외부 데이터 소스(벡터DB)와의 연동을 용이하게 합니다.
2. 설치 및 환경 설정 Python 환경에 LangChain 및 관련 라이브러리를 설치합니다.
Bash

pip install langchain langchain-openai pydantic faiss-cpu tiktoken
pip install pinecone-client # Pinecone 사용 시
환경 변수에 OpenAI API Key를 설정합니다.
Bash

export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
3. 문서 전처리 및 임베딩 RAG의 핵심은 문서 DB 구축입니다.
	•	문서 로드: PDF, DOCX, TXT 등 다양한 형식의 문서를 로드합니다. langchain.document_loaders 모듈을 사용합니다. Pythonfrom langchain.document_loaders import PyPDFLoader
	•	loader = PyPDFLoader("path/to/your/document.pdf")
	•	documents = loader.load()
	•	
	•	문서 분할 (Text Splitting): LLM의 토큰 제한 및 검색 효율성을 위해 문서를 적절한 크기로 분할합니다. Pythonfrom langchain.text_splitter import RecursiveCharacterTextSplitter
	•	text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
	•	splits = text_splitter.split_documents(documents)
	•	
	•	임베딩 생성: 분할된 텍스트 청크를 임베딩 벡터로 변환합니다. OpenAIEmbeddings 또는 다른 임베딩 모델을 사용합니다. Pythonfrom langchain_openai import OpenAIEmbeddings
	•	embeddings = OpenAIEmbeddings()
	•	
	•	벡터 DB 저장: 생성된 임베딩과 원본 텍스트를 벡터 데이터베이스(Pinecone, FAISS 등)에 저장합니다. Pythonfrom langchain_pinecone import PineconeVectorStore
	•	# Pinecone 초기화 (API Key, Environment 설정 필요)
	•	# index_name = "smart-factory-chatbot"
	•	# docsearch = PineconeVectorStore.from_documents(splits, embeddings, index_name=index_name)
	•	
	•	# 또는 FAISS 로컬 저장 (간편한 테스트용)
	•	from langchain_community.vectorstores import FAISS
	•	vectorstore = FAISS.from_documents(splits, embeddings)
	•	vectorstore.save_local("faiss_index")
	•	
4. RAG 체인 구성 질의응답을 위한 RAG 체인을 구성합니다.
	•	리트리버 (Retriever): 벡터 DB에서 사용자 질의와 관련된 문서를 검색합니다. Python# Pinecone 사용 시:
	•	# retriever = PineconeVectorStore(index_name=index_name, embedding=embeddings).as_retriever()
	•	# FAISS 사용 시:
	•	vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
	•	retriever = vectorstore.as_retriever()
	•	
	•	LLM (Large Language Model): 검색된 문서와 사용자 질의를 바탕으로 답변을 생성합니다. Pythonfrom langchain_openai import ChatOpenAI
	•	llm = ChatOpenAI(model_name="gpt-4o", temperature=0) # 최신 모델 사용 권장
	•	
	•	프롬프트 템플릿: LLM에 전달할 프롬프트 형식을 정의합니다. 검색된 문서와 질의를 포함합니다. Pythonfrom langchain.prompts import ChatPromptTemplate
	•	template = """다음 맥락 정보를 사용하여 질문에 답변하세요:
	•	{context}
	•	
	•	질문: {question}
	•	"""
	•	prompt = ChatPromptTemplate.from_template(template)
	•	
	•	체인 조립: LCEL(LangChain Expression Language)을 사용하여 체인을 조립합니다. 코드 스니펫from langchain.schema.output_parser import StrOutputParser
	•	from langchain.schema.runnable import RunnablePassthrough
	•	
	•	rag_chain = (
	•	    {"context": retriever, "question": RunnablePassthrough()}
	•	    | prompt
	•	    | llm
	•	    | StrOutputParser()
	•	)
	•	
5. 챗봇 인터랙션 구현 사용자 질의를 받아 RAG 체인을 호출하고 응답을 반환합니다.
Python

def get_chatbot_response(question: str, image_context: list = None):
    # 이미지 컨텍스트는 현재 텍스트 기반 RAG에 직접 활용하기 어려우므로,
    # 필요시 이미지 분석 모델(예: GPT-4o Vision)을 통해 텍스트 설명으로 변환 후 활용
    # 또는 이미지 자체를 임베딩하여 멀티모달 RAG로 확장 (향후 Phase 2/3 고려)

    response = rag_chain.invoke(question)
    return response

# 예시 사용
# user_question = "설비 A의 온도가 급격히 상승하는데 원인이 무엇인가요?"
# chatbot_answer = get_chatbot_response(user_question)
# print(chatbot_answer)
6. 메모리 관리 (대화 이력) 챗봇이 이전 대화의 맥락을 기억하도록 ConversationBufferMemory와 같은 메모리 모듈을 사용할 수 있습니다.
Python

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
conversation_chain = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)

# 대화 예시
# result = conversation_chain.invoke({"question": "최근 발생한 과열 이슈에 대해 알려줘."})
# print(result["answer"])
# result = conversation_chain.invoke({"question": "그 문제의 해결책은 무엇이었어?"})
# print(result["answer"])
7. 성능 최적화 고려사항
	•	청킹 전략: 문서 유형에 따라 적절한 chunk_size와 chunk_overlap 설정이 중요합니다.
	•	임베딩 모델: 도메인 특화된 임베딩 모델 사용을 고려할 수 있습니다.
	•	리트리버 튜닝: Top-K 검색 결과 수, 벡터DB 인덱스 최적화 등이 성능에 영향을 미칩니다.
	•	프롬프트 엔지니어링: LLM의 응답 품질을 높이기 위해 프롬프트 템플릿을 지속적으로 개선합니다.
8. 트러블슈팅
	•	응답이 부정확할 때: context에 관련 정보가 충분히 포함되는지 확인합니다. 문서 전처리 및 임베딩 과정을 점검합니다.
	•	토큰 제한 오류: chunk_size를 줄이거나, 더 긴 컨텍스트를 처리할 수 있는 LLM 모델을 사용합니다.
